 

Summary
john detailed their CI/CD pipeline encompassing various build and deployment processes for different application types, their use of HashiCorp Vault for secrets management, and the ongoing migration of GitLab Runners and other tools to SAS platforms. Their AWS infrastructure is managed via Control Tower with Brownfield and Greenfield environments, utilizing in-house Terraform modules and a Service Catalog for resource provisioning, alongside a centrally managed backup strategy and network connectivity via a transit gateway. john also covered their cloud governance practices, including mandatory tagging enforced by AWS Config rules, and their Kubernetes deployments in AWS EKS leveraging GitLab CI/CD and IAM roles.
Details
•	CI/CD Pipeline john described their CI/CD pipeline, which includes stages for code checkout from GitLab, building with Maven or Gradle, unit testing, SonarQube code scanning (requiring >80% coverage), building a Docker image, scanning the Docker image for vulnerabilities, and pushing to Artifactory (00:00:00). For microservices, deployment involves Helm and Kubernetes, while monolithic applications use GitLab for EC2 deployments (00:01:12). Front-end applications using AngularJS, NodeJS, or ReactJS are built with npm, pushed to Artifactory as a gzipped file, and deployed to S3 or EC2 (00:02:40). .NET applications use MS Build and deploy to Windows servers, while Python applications may use Maven (00:03:56). Lambda deployments involve zipping the code and deploying from Artifactory (00:05:22).
•	HashiCorp Vault and GitLab Runners john explained that HashiCorp Vault is used to store and rotate sensitive information like database credentials, which applications retrieve at runtime. The GitLab runners, which execute pipeline jobs, are self-hosted and are being migrated to a SAS platform (00:06:35). john clarified that even with a SAS platform, maintaining the GitLab runners in their environment is necessary, and these runners are Docker containers that spin up for each job and terminate upon completion (00:07:59). GitLab and JFrog Artifactory have also been migrated to SAS platforms (00:09:04).
•	AWS Control Tower Environment john detailed their cloud infrastructure managed by a central team, which includes both Brownfield (legacy, upgraded to Control Tower) and Greenfield (new) environments (00:09:04). The Control Tower Account Factory is used to provision new AWS accounts after the Security Architecture Team (SAT) approves the application architecture (00:10:19). They utilize 38 Terraform modules developed in-house to provision infrastructure in new development and test environments, and application code is deployed using their CI/CD pipelines (00:11:38). Promotion to production requires further SAT approval and involves creating new AWS accounts and deploying infrastructure and code again (00:12:53).
•	Control Tower Customizations and Service Catalog john described how Control Tower customizations, leveraging native AWS CI/CD tools, enforce best practices like service control policies and backup policies across new accounts (00:12:53). The event-driven process involves CloudTrail, EventBridge, SQS, Lambda, and CodePipeline (00:14:01). They also utilize a Service Catalog with three portfolios (IAM, network, and other AWS resources) containing over 50 Terraform-based products, making it easier for development teams to provision resources like load balancers and EC2 instances (00:15:18).
•	AWS Backup and Network Connectivity john explained their AWS Backup strategy, where backup policies are created in the control tower and applied to organizational units, becoming backup plans in member accounts. They support backups for seven services, and end users simply need to tag critical resources to initiate backups within the same account, with different retention periods for production (90 days) and non-production (30 days) (00:16:43). john also discussed their network connectivity, which utilizes a transit gateway and direct connect to their two on-premise data centers for private communication between the cloud and on-premise environments (00:19:00). Disaster recovery is limited to a few applications in a secondary region (00:20:15).
•	Cloud Governance and Tagging Strategy john outlined their cloud governance practices, including a mandatory tagging policy where six business tags are collected during account creation and stored in SSM Parameter Store. AWS Config rules monitor resources for these tags, and a Lambda function automatically tags non-compliant resources by fetching the values from SSM (00:21:19). They also have Config rules to enforce EBS volume and S3 encryption and restrict the creation of public-facing REST API Gateways (00:22:26).
•	Kubernetes Deployment in AWS EKS john described their Kubernetes deployment process in AWS EKS, where each service gets an ingress object in addition to deployment, replica set, and service manifests (00:23:46). They utilize a load balancer controller deployed during EKS cluster creation to automatically create AWS load balancers for each service. Traffic flow involves Route 53, API Gateway, load balancer, and finally the pod (00:25:19). They primarily use GitLab for CI/CD and leverage IAM roles to allow GitLab runners to authenticate with and deploy to EKS clusters, utilizing Helm templates during the deployment stage (00:26:27).
Suggested next steps
No suggested next steps were found for this meeting.


